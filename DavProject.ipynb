{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0f5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to C:\\Users\\DELL\\.convokit\\downloads\\movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "#pip install convokit\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))\n",
    "\n",
    "#corpus.print_summary_stats()\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from convokit import Corpus, download\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from convokit import Corpus, download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8803cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conversation_ID</th>\n",
       "      <th>Movie_Index</th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Conversation_ID_Utterance</th>\n",
       "      <th>Reply_To</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L984</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>L984</td>\n",
       "      <td>u2</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L924</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>1999</td>\n",
       "      <td>6.90</td>\n",
       "      <td>62847</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304708</th>\n",
       "      <td>L666369</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>1979</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1911</td>\n",
       "      <td>['action', 'adventure', 'drama', 'history', 'w...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>u9030</td>\n",
       "      <td>L666369</td>\n",
       "      <td>L666370</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304709</th>\n",
       "      <td>L666369</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>1979</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1911</td>\n",
       "      <td>['action', 'adventure', 'drama', 'history', 'w...</td>\n",
       "      <td>L666370</td>\n",
       "      <td>u9034</td>\n",
       "      <td>L666369</td>\n",
       "      <td>L666369</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304710</th>\n",
       "      <td>L666369</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>1979</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1911</td>\n",
       "      <td>['action', 'adventure', 'drama', 'history', 'w...</td>\n",
       "      <td>L666369</td>\n",
       "      <td>u9030</td>\n",
       "      <td>L666369</td>\n",
       "      <td>None</td>\n",
       "      <td>Your orders, Mr Vereker?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304711</th>\n",
       "      <td>L666256</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>1979</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1911</td>\n",
       "      <td>['action', 'adventure', 'drama', 'history', 'w...</td>\n",
       "      <td>L666257</td>\n",
       "      <td>u9030</td>\n",
       "      <td>L666256</td>\n",
       "      <td>L666256</td>\n",
       "      <td>Good ones, yes, Mr Vereker. Gentlemen who can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304712</th>\n",
       "      <td>L666256</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>1979</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1911</td>\n",
       "      <td>['action', 'adventure', 'drama', 'history', 'w...</td>\n",
       "      <td>L666256</td>\n",
       "      <td>u9034</td>\n",
       "      <td>L666256</td>\n",
       "      <td>None</td>\n",
       "      <td>Colonel Durnford... William Vereker. I hear yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304713 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Conversation_ID Movie_Index                  Movie_Name Release_Year  \\\n",
       "0                L1044          m0  10 things i hate about you         1999   \n",
       "1                L1044          m0  10 things i hate about you         1999   \n",
       "2                 L984          m0  10 things i hate about you         1999   \n",
       "3                 L984          m0  10 things i hate about you         1999   \n",
       "4                 L924          m0  10 things i hate about you         1999   \n",
       "...                ...         ...                         ...          ...   \n",
       "304708         L666369        m616                   zulu dawn         1979   \n",
       "304709         L666369        m616                   zulu dawn         1979   \n",
       "304710         L666369        m616                   zulu dawn         1979   \n",
       "304711         L666256        m616                   zulu dawn         1979   \n",
       "304712         L666256        m616                   zulu dawn         1979   \n",
       "\n",
       "       Rating  Votes                                              Genre  \\\n",
       "0        6.90  62847                              ['comedy', 'romance']   \n",
       "1        6.90  62847                              ['comedy', 'romance']   \n",
       "2        6.90  62847                              ['comedy', 'romance']   \n",
       "3        6.90  62847                              ['comedy', 'romance']   \n",
       "4        6.90  62847                              ['comedy', 'romance']   \n",
       "...       ...    ...                                                ...   \n",
       "304708   6.40   1911  ['action', 'adventure', 'drama', 'history', 'w...   \n",
       "304709   6.40   1911  ['action', 'adventure', 'drama', 'history', 'w...   \n",
       "304710   6.40   1911  ['action', 'adventure', 'drama', 'history', 'w...   \n",
       "304711   6.40   1911  ['action', 'adventure', 'drama', 'history', 'w...   \n",
       "304712   6.40   1911  ['action', 'adventure', 'drama', 'history', 'w...   \n",
       "\n",
       "       Utterance_ID Speaker Conversation_ID_Utterance Reply_To  \\\n",
       "0             L1045      u0                     L1044    L1044   \n",
       "1             L1044      u2                     L1044     None   \n",
       "2              L985      u0                      L984     L984   \n",
       "3              L984      u2                      L984     None   \n",
       "4              L925      u0                      L924     L924   \n",
       "...             ...     ...                       ...      ...   \n",
       "304708      L666371   u9030                   L666369  L666370   \n",
       "304709      L666370   u9034                   L666369  L666369   \n",
       "304710      L666369   u9030                   L666369     None   \n",
       "304711      L666257   u9030                   L666256  L666256   \n",
       "304712      L666256   u9034                   L666256     None   \n",
       "\n",
       "                                                     Text  \n",
       "0                                            They do not!  \n",
       "1                                             They do to!  \n",
       "2                                              I hope so.  \n",
       "3                                               She okay?  \n",
       "4                                               Let's go.  \n",
       "...                                                   ...  \n",
       "304708  Lord Chelmsford seems to want me to stay back ...  \n",
       "304709  I'm to take the Sikali with the main column to...  \n",
       "304710                           Your orders, Mr Vereker?  \n",
       "304711  Good ones, yes, Mr Vereker. Gentlemen who can ...  \n",
       "304712  Colonel Durnford... William Vereker. I hear yo...  \n",
       "\n",
       "[304713 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#making the dataFrame\n",
    "\n",
    "# Initialize lists to store data\n",
    "convo_ids = []\n",
    "movie_indices = []\n",
    "movie_names = []\n",
    "release_years = []\n",
    "ratings = []\n",
    "votes = []\n",
    "genres = []\n",
    "utterance_ids = []\n",
    "speakers = []\n",
    "conversation_ids = []\n",
    "reply_tos = []\n",
    "\n",
    "texts = []\n",
    "\n",
    "#Iterate through conversations to extract data\n",
    "for conv_id in corpus.get_conversation_ids():\n",
    "    conversation = corpus.get_conversation(conv_id)\n",
    "    movie_idx = conversation.meta['movie_idx']\n",
    "    movie_name = conversation.meta['movie_name']\n",
    "    release_year = conversation.meta['release_year']\n",
    "    rating = conversation.meta['rating']\n",
    "    vote = conversation.meta['votes']\n",
    "    genre = conversation.meta['genre']\n",
    "   \n",
    "    \n",
    "    for utt in conversation.iter_utterances():\n",
    "        convo_ids.append(conv_id)\n",
    "        movie_indices.append(movie_idx)\n",
    "        movie_names.append(movie_name)\n",
    "        release_years.append(release_year)\n",
    "        ratings.append(rating)\n",
    "        votes.append(vote)\n",
    "        genres.append(genre)\n",
    "        utterance_ids.append(utt.id)\n",
    "        speakers.append(utt.speaker.id)\n",
    "        conversation_ids.append(utt.conversation_id)\n",
    "        reply_tos.append(utt.reply_to)\n",
    "       \n",
    "        texts.append(utt.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Conversation_ID': convo_ids,\n",
    "    'Movie_Index': movie_indices,\n",
    "    'Movie_Name': movie_names,\n",
    "    'Release_Year': release_years,\n",
    "    'Rating': ratings,\n",
    "    'Votes': votes,\n",
    "    'Genre': genres,\n",
    "    'Utterance_ID': utterance_ids,\n",
    "    'Speaker': speakers,\n",
    "    'Conversation_ID_Utterance': conversation_ids,\n",
    "    'Reply_To': reply_tos,\n",
    "    \n",
    "    'Text': texts\n",
    "})\n",
    "\n",
    "\n",
    "df.dropna().drop_duplicates()\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fa5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_emotion(dialogue):\n",
    "    # Create a SentimentIntensityAnalyzer instance\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Perform sentiment analysis on the dialogue\n",
    "    sentiment = sia.polarity_scores(dialogue)\n",
    "\n",
    "    # Determine emotion based on sentiment scores\n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        emotion = 'positive'\n",
    "    elif sentiment['compound'] <= -0.05:\n",
    "        emotion = 'negative'\n",
    "    else:\n",
    "        emotion = 'neutral'\n",
    "\n",
    "    return emotion\n",
    "\n",
    "def add_sentiment_column(df):\n",
    "    # Create a SentimentIntensityAnalyzer instance\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Create an empty list to store emotions\n",
    "    emotions = []\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Perform sentiment analysis on the dialogue\n",
    "        sentiment = sia.polarity_scores(row['Text'])\n",
    "\n",
    "        # Determine emotion based on sentiment scores\n",
    "        if sentiment['compound'] >= 0.05:\n",
    "            emotion = 'positive'\n",
    "        elif sentiment['compound'] <= -0.05:\n",
    "            emotion = 'negative'\n",
    "        else:\n",
    "            emotion = 'neutral'\n",
    "\n",
    "        # Store emotion in the list\n",
    "        emotions.append(emotion)\n",
    "\n",
    "    # Add a new column to the DataFrame with emotions\n",
    "    df['Emotion'] = emotions\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'YourMovieTitle' is the movie you want to analyze\n",
    "movie_to_analyze = df[df['Movie_Name'] == 'YourMovieTitle']\n",
    "\n",
    "# Add a hypothetical 'Timestamp' column representing the temporal aspect\n",
    "movie_to_analyze['Timestamp'] = range(1, len(movie_to_analyze) + 1)\n",
    "\n",
    "# Add emotion column to the entire DataFrame\n",
    "add_sentiment_column(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a620273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('df.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0670a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = \"C:\\\\Users\\\\DELL\\\\Desktop\\\\VisProject\\\\df.csv\"\n",
    "\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv(dfs, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "244973ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a dialogue: Your orders, Mr Vereker?\n",
      "Movie: zulu dawn\n",
      "Rating: 6.40\n",
      "Emotion of the dialogue is: neutral\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for a dialogue\n",
    "user_dialogue = input(\"Enter a dialogue: \")\n",
    "\n",
    "# Check if the dialogue exists in the DataFrame\n",
    "result = df[df['Text'] == user_dialogue]\n",
    "\n",
    "if not result.empty:\n",
    "    # Extract relevant information\n",
    "    movie_name = result['Movie_Name'].values[0]\n",
    "    rating = result['Rating'].values[0]\n",
    "    emotion = result['Emotion'].values[0]\n",
    "\n",
    "    print(f\"Movie: {movie_name}\")\n",
    "    print(f\"Rating: {rating}\")\n",
    "    print(f\"Emotion of the dialogue is: {emotion}\")\n",
    "else:\n",
    "    print(\"Dialogue not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf888b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Character  Frequency                  Movie_Name Speaker  \\\n",
      "0                 Joey          8  10 things i hate about you      u7   \n",
      "1              Cameron          7  10 things i hate about you      u7   \n",
      "2              Patrick          5  10 things i hate about you      u8   \n",
      "3              William          4  10 things i hate about you      u6   \n",
      "4       Sarah Lawrence          4  10 things i hate about you     u11   \n",
      "...                ...        ...                         ...     ...   \n",
      "23079            Noggs          1                   zulu dawn   u9027   \n",
      "23080           Bugler          1                   zulu dawn   u9025   \n",
      "23081              Boy          1                   zulu dawn   u9025   \n",
      "23082             Cook          1                   zulu dawn   u9026   \n",
      "23083  William Vereker          1                   zulu dawn   u9034   \n",
      "\n",
      "      Reply_To                                           Dialogue  \n",
      "0         L381  Patrick, Pat, you're not looking at the big pi...  \n",
      "1         L375  The situation is, my man Cameron here has a ma...  \n",
      "2         None  I don't understand, Patrick.  You haven't done...  \n",
      "3         L151  You could always go with me.  I'm sure William...  \n",
      "4         L170  Sarah Lawrence is on the other side of the cou...  \n",
      "...        ...                                                ...  \n",
      "23079     None                      What o'clock is it, Mr Noggs?  \n",
      "23080  L666361  I listened extra careful to your \"Stand To\" th...  \n",
      "23081  L666361  I listened extra careful to your \"Stand To\" th...  \n",
      "23082  L666362  From the Cook, Sir They saw me dip your shavin...  \n",
      "23083     None  Colonel Durnford... William Vereker. I hear yo...  \n",
      "\n",
      "[23084 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#new character dataset for the entire dataset\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming your movie DataFrame is named df\n",
    "# ...\n",
    "\n",
    "# Function to perform NER on the entire dataset\n",
    "def perform_ner_on_dataset(df):\n",
    "    # Create dictionaries to store character frequencies along with speaker, reply_to, and dialogue\n",
    "    character_counts = {}\n",
    "    speaker_dict = {}\n",
    "    reply_to_dict = {}\n",
    "    dialogue_dict = {}\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Process the dialogue using spaCy\n",
    "        doc = nlp(row['Text'])\n",
    "\n",
    "        # Extract named entities (characters) from the dialogue\n",
    "        characters = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "       \n",
    "        # Count the occurrences of each character\n",
    "        for character in characters:\n",
    "            if character in character_counts:\n",
    "                character_counts[character] += 1\n",
    "            else:\n",
    "                character_counts[character] = 1\n",
    "\n",
    "            # Store speaker, reply_to, and dialogue information\n",
    "            speaker_dict[character] = row['Speaker']\n",
    "            reply_to_dict[character] = row['Reply_To']\n",
    "            dialogue_dict[character] = row[\"Text\"]\n",
    "\n",
    "    # Create a DataFrame from the dictionaries\n",
    "    df_result = pd.DataFrame(list(character_counts.items()), columns=['Character', 'Frequency'])\n",
    "\n",
    "    # Sort characters based on frequency in descending order\n",
    "    df_result = df_result.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "    # Add columns for movie name, speaker, reply_to, and dialogue\n",
    "    df_result['Movie_Name'] = df['Movie_Name'].iloc[0]\n",
    "    df_result['Speaker'] = df_result['Character'].map(speaker_dict)\n",
    "    df_result['Reply_To'] = df_result['Character'].map(reply_to_dict)\n",
    "    df_result['Dialogue'] = df_result['Character'].map(dialogue_dict)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Create a list to store DataFrames for each movie\n",
    "dfs = []\n",
    "\n",
    "# Iterate through unique movie names in the DataFrame\n",
    "for movie_name in df['Movie_Name'].unique():\n",
    "    # Perform NER on dialogues of each movie\n",
    "    character_frequencies_movie = perform_ner_on_dataset(df[df['Movie_Name'] == movie_name])\n",
    "    \n",
    "    # Append the result to the list\n",
    "    dfs.append(character_frequencies_movie)\n",
    "\n",
    "# Concatenate the DataFrames in the list\n",
    "df_final_result = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89e5c839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freqchar = \"C:\\\\Users\\\\DELL\\\\Desktop\\\\VisProject\\\\freqchar.csv\"\n",
    "\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_final_result.to_csv(freqchar, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17590f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "387a2fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Reply_To</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joey</td>\n",
       "      <td>8</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>u7</td>\n",
       "      <td>L381</td>\n",
       "      <td>Patrick, Pat, you're not looking at the big pi...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cameron</td>\n",
       "      <td>7</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>u7</td>\n",
       "      <td>L375</td>\n",
       "      <td>The situation is, my man Cameron here has a ma...</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patrick</td>\n",
       "      <td>5</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>u8</td>\n",
       "      <td>None</td>\n",
       "      <td>I don't understand, Patrick.  You haven't done...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>William</td>\n",
       "      <td>4</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>u6</td>\n",
       "      <td>L151</td>\n",
       "      <td>You could always go with me.  I'm sure William...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Lawrence</td>\n",
       "      <td>4</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>u11</td>\n",
       "      <td>L170</td>\n",
       "      <td>Sarah Lawrence is on the other side of the cou...</td>\n",
       "      <td>-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23079</th>\n",
       "      <td>Noggs</td>\n",
       "      <td>1</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>u9027</td>\n",
       "      <td>None</td>\n",
       "      <td>What o'clock is it, Mr Noggs?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23080</th>\n",
       "      <td>Bugler</td>\n",
       "      <td>1</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>u9025</td>\n",
       "      <td>L666361</td>\n",
       "      <td>I listened extra careful to your \"Stand To\" th...</td>\n",
       "      <td>0.058333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23081</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>u9025</td>\n",
       "      <td>L666361</td>\n",
       "      <td>I listened extra careful to your \"Stand To\" th...</td>\n",
       "      <td>0.058333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23082</th>\n",
       "      <td>Cook</td>\n",
       "      <td>1</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>u9026</td>\n",
       "      <td>L666362</td>\n",
       "      <td>From the Cook, Sir They saw me dip your shavin...</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23083</th>\n",
       "      <td>William Vereker</td>\n",
       "      <td>1</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>u9034</td>\n",
       "      <td>None</td>\n",
       "      <td>Colonel Durnford... William Vereker. I hear yo...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23084 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Character  Frequency                  Movie_Name Speaker  \\\n",
       "0                 Joey          8  10 things i hate about you      u7   \n",
       "1              Cameron          7  10 things i hate about you      u7   \n",
       "2              Patrick          5  10 things i hate about you      u8   \n",
       "3              William          4  10 things i hate about you      u6   \n",
       "4       Sarah Lawrence          4  10 things i hate about you     u11   \n",
       "...                ...        ...                         ...     ...   \n",
       "23079            Noggs          1                   zulu dawn   u9027   \n",
       "23080           Bugler          1                   zulu dawn   u9025   \n",
       "23081              Boy          1                   zulu dawn   u9025   \n",
       "23082             Cook          1                   zulu dawn   u9026   \n",
       "23083  William Vereker          1                   zulu dawn   u9034   \n",
       "\n",
       "      Reply_To                                           Dialogue  Sentiment  \n",
       "0         L381  Patrick, Pat, you're not looking at the big pi...   0.100000  \n",
       "1         L375  The situation is, my man Cameron here has a ma...   0.062500  \n",
       "2         None  I don't understand, Patrick.  You haven't done...   0.000000  \n",
       "3         L151  You could always go with me.  I'm sure William...   0.500000  \n",
       "4         L170  Sarah Lawrence is on the other side of the cou...  -0.125000  \n",
       "...        ...                                                ...        ...  \n",
       "23079     None                      What o'clock is it, Mr Noggs?   0.000000  \n",
       "23080  L666361  I listened extra careful to your \"Stand To\" th...   0.058333  \n",
       "23081  L666361  I listened extra careful to your \"Stand To\" th...   0.058333  \n",
       "23082  L666362  From the Cook, Sir They saw me dip your shavin...  -0.016667  \n",
       "23083     None  Colonel Durnford... William Vereker. I hear yo...   0.000000  \n",
       "\n",
       "[23084 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7c56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "984856cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Character'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m df_final_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_final_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Group by character and calculate average sentiment\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m character_sentiments \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCharacter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the aggregated emotions for each character\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(character_sentiments)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7712\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7707\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m   7709\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[0;32m   7710\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[0;32m   7711\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[1;32m-> 7712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7715\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7718\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7720\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   7721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:882\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    880\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Character'"
     ]
    }
   ],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Add a new column for sentiment analysis using TextBlob\n",
    "df_final_result['Sentiment'] = df_final_result['Dialogue'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Group by character and calculate average sentiment\n",
    "character_sentiments = df.groupby('Character')['Sentiment'].mean()\n",
    "\n",
    "# Print the aggregated emotions for each character\n",
    "print(character_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee6d0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Replied_To</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>okay</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>okay</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u0</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okay</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u2</td>\n",
       "      <td>L196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>okay</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u2</td>\n",
       "      <td>L194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay</td>\n",
       "      <td>Okay!  I wasn't sure</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u2</td>\n",
       "      <td>L424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>big</td>\n",
       "      <td>I know.  It'd have to be a pretty big deal to ...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u9</td>\n",
       "      <td>L642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>big</td>\n",
       "      <td>Leave it to you to use big words when you're s...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u9</td>\n",
       "      <td>L626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>big</td>\n",
       "      <td>You're not a big talker, are you?</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>big</td>\n",
       "      <td>Yeah.  She left with some bikers Big ones.  Fu...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u5</td>\n",
       "      <td>L986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>big</td>\n",
       "      <td>Patrick, Pat, you're not looking at the big pi...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>6.90</td>\n",
       "      <td>u7</td>\n",
       "      <td>L381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word                                           Dialogue  \\\n",
       "0    okay                                          She okay?   \n",
       "1    okay     Okay -- you're gonna need to learn how to lie.   \n",
       "2    okay  Okay... then how 'bout we try out some French ...   \n",
       "3    okay  Well, I thought we'd start with pronunciation,...   \n",
       "4    okay                               Okay!  I wasn't sure   \n",
       "..    ...                                                ...   \n",
       "888   big  I know.  It'd have to be a pretty big deal to ...   \n",
       "889   big  Leave it to you to use big words when you're s...   \n",
       "890   big                  You're not a big talker, are you?   \n",
       "891   big  Yeah.  She left with some bikers Big ones.  Fu...   \n",
       "892   big  Patrick, Pat, you're not looking at the big pi...   \n",
       "\n",
       "                     Movie_Name Rating Speaker Replied_To  \n",
       "0    10 things i hate about you   6.90      u2       None  \n",
       "1    10 things i hate about you   6.90      u0       L871  \n",
       "2    10 things i hate about you   6.90      u2       L196  \n",
       "3    10 things i hate about you   6.90      u2       L194  \n",
       "4    10 things i hate about you   6.90      u2       L424  \n",
       "..                          ...    ...     ...        ...  \n",
       "888  10 things i hate about you   6.90      u9       L642  \n",
       "889  10 things i hate about you   6.90      u9       L626  \n",
       "890  10 things i hate about you   6.90      u9       None  \n",
       "891  10 things i hate about you   6.90      u5       L986  \n",
       "892  10 things i hate about you   6.90      u7       L381  \n",
       "\n",
       "[893 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new counts \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming your movie DataFrame is named df\n",
    "# ...\n",
    "\n",
    "# Function to perform NLP analysis on a specific movie from the dataset\n",
    "def analyze_movie(df, movie_name):\n",
    "    # Filter the DataFrame to get dialogues of the specified movie\n",
    "    movie_df = df[df['Movie_Name'] == movie_name]\n",
    "\n",
    "    # Concatenate all dialogues into a single text for the movie\n",
    "    movie_text = ' '.join(movie_df['Text'])\n",
    "\n",
    "    # Process the concatenated text using spaCy\n",
    "    doc = nlp(movie_text)\n",
    "\n",
    "    # Extract words from the text, excluding stop words, articles, and one-character words\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha and\n",
    "             token.text.lower() not in nlp.Defaults.stop_words and\n",
    "             token.pos_ != 'DET' and len(token.text) > 1]\n",
    "\n",
    "    # Count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Filter words based on the threshold (more than 5 occurrences)\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if count > 5}\n",
    "\n",
    "    # Create a list to store word-dialogue pairs with additional information\n",
    "    word_dialogue_list = []\n",
    "\n",
    "    # Iterate through filtered words and find dialogues\n",
    "    for word in filtered_words.keys():\n",
    "        dialogues = [dialogue for dialogue in movie_df[movie_df['Text'].str.lower().str.contains(word)]['Text']]\n",
    "        for dialogue in dialogues:\n",
    "            # Extract additional information from the corresponding row in movie_df\n",
    "            row = movie_df[movie_df['Text'] == dialogue].iloc[0]\n",
    "            word_dialogue_list.append({\n",
    "                'Word': word,\n",
    "                'Dialogue': dialogue,\n",
    "                'Movie_Name': row['Movie_Name'],\n",
    "                'Rating': row['Rating'],\n",
    "                'Speaker': row['Speaker'],\n",
    "                'Replied_To': row['Reply_To']\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    df_word_dialogues = pd.DataFrame(word_dialogue_list)\n",
    "\n",
    "    return df_word_dialogues\n",
    "\n",
    "# Get the name and rating of the first movie in the dataset\n",
    "first_movie_name = df['Movie_Name'].iloc[0]\n",
    "first_movie_rating = df['Rating'].iloc[0]\n",
    "\n",
    "# Analyze dialogues of the first movie\n",
    "df_word_dialogues_first_movie = analyze_movie(df, first_movie_name)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "df_word_dialogues_first_movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc142f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word                                           Dialogue  \\\n",
      "0       okay                                          She okay?   \n",
      "1       okay     Okay -- you're gonna need to learn how to lie.   \n",
      "2       okay  Okay... then how 'bout we try out some French ...   \n",
      "3       okay  Well, I thought we'd start with pronunciation,...   \n",
      "4       okay                               Okay!  I wasn't sure   \n",
      "...      ...                                                ...   \n",
      "570145  lord  Excuse me, My Lord.  Norris-Newman, of \"The St...   \n",
      "570146  lord  My Lord.  This list was prepared for you. I do...   \n",
      "570147  lord  Sikali Horse, My Lord. Christians alL I know e...   \n",
      "570148  lord  Um. There are rumours that my Lord Chelmsford ...   \n",
      "570149  lord  Lord Chelmsford seems to want me to stay back ...   \n",
      "\n",
      "                        Movie_Name Rating Speaker Replied_To  Frequency  \n",
      "0       10 things i hate about you   6.90      u2       None         10  \n",
      "1       10 things i hate about you   6.90      u0       L871         10  \n",
      "2       10 things i hate about you   6.90      u2       L196         10  \n",
      "3       10 things i hate about you   6.90      u2       L194         10  \n",
      "4       10 things i hate about you   6.90      u2       L424         10  \n",
      "...                            ...    ...     ...        ...        ...  \n",
      "570145                   zulu dawn   6.40   u9032    L666388          9  \n",
      "570146                   zulu dawn   6.40   u9030    L666357          9  \n",
      "570147                   zulu dawn   6.40   u9030    L666246          9  \n",
      "570148                   zulu dawn   6.40   u9028    L666262          9  \n",
      "570149                   zulu dawn   6.40   u9030    L666370          9  \n",
      "\n",
      "[570150 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming your movie DataFrame is named df\n",
    "# ...\n",
    "\n",
    "# Function to perform NLP analysis on the entire dataset\n",
    "def analyze_dataset(df):\n",
    "    # Create a list to store word-dialogue pairs with additional information\n",
    "    word_dialogue_list = []\n",
    "\n",
    "    # Iterate through each movie in the dataset\n",
    "    for movie_name in df['Movie_Name'].unique():\n",
    "        # Filter the DataFrame to get dialogues of the specified movie\n",
    "        movie_df = df[df['Movie_Name'] == movie_name]\n",
    "\n",
    "        # Concatenate all dialogues into a single text for the movie\n",
    "        movie_text = ' '.join(movie_df['Text'])\n",
    "\n",
    "        # Process the concatenated text using spaCy\n",
    "        doc = nlp(movie_text)\n",
    "\n",
    "        # Extract words from the text, excluding stop words, articles, and one-character words\n",
    "        words = [token.text.lower() for token in doc if token.is_alpha and\n",
    "                 token.text.lower() not in nlp.Defaults.stop_words and\n",
    "                 token.pos_ != 'DET' and len(token.text) > 1]\n",
    "\n",
    "        # Count the occurrences of each word\n",
    "        word_counts = Counter(words)\n",
    "\n",
    "        # Filter words based on the threshold (more than 5 occurrences)\n",
    "        filtered_words = {word: count for word, count in word_counts.items() if count > 5}\n",
    "\n",
    "        # Iterate through filtered words and find dialogues\n",
    "        for word, frequency in filtered_words.items():\n",
    "            dialogues = [dialogue for dialogue in movie_df[movie_df['Text'].str.lower().str.contains(word)]['Text']]\n",
    "            for dialogue in dialogues:\n",
    "                # Extract additional information from the corresponding row in movie_df\n",
    "                row = movie_df[movie_df['Text'] == dialogue].iloc[0]\n",
    "                word_dialogue_list.append({\n",
    "                    'Word': word,\n",
    "                    'Dialogue': dialogue,\n",
    "                    'Movie_Name': row['Movie_Name'],\n",
    "                    'Rating': row['Rating'],\n",
    "                    'Speaker': row['Speaker'],\n",
    "                    'Replied_To': row['Reply_To'],\n",
    "                    'Frequency': frequency\n",
    "                })\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    df_word_dialogues = pd.DataFrame(word_dialogue_list)\n",
    "\n",
    "    return df_word_dialogues\n",
    "\n",
    "# Analyze dialogues for the entire dataset\n",
    "df_word_dialogues_entire_dataset = analyze_dataset(df)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_word_dialogues_entire_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd833886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Movie_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>okay</td>\n",
       "      <td>10</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>let</td>\n",
       "      <td>13</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>need</td>\n",
       "      <td>12</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>know</td>\n",
       "      <td>43</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>like</td>\n",
       "      <td>43</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569545</th>\n",
       "      <td>mutants</td>\n",
       "      <td>8</td>\n",
       "      <td>x-men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569755</th>\n",
       "      <td>fronkonsteen</td>\n",
       "      <td>7</td>\n",
       "      <td>young frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569861</th>\n",
       "      <td>mmmmm</td>\n",
       "      <td>10</td>\n",
       "      <td>young frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569974</th>\n",
       "      <td>delbruck</td>\n",
       "      <td>6</td>\n",
       "      <td>young frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570123</th>\n",
       "      <td>zulu</td>\n",
       "      <td>7</td>\n",
       "      <td>zulu dawn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4959 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Frequency                  Movie_Name\n",
       "0               okay         10  10 things i hate about you\n",
       "9                let         13  10 things i hate about you\n",
       "31              need         12  10 things i hate about you\n",
       "44              know         43  10 things i hate about you\n",
       "88              like         43  10 things i hate about you\n",
       "...              ...        ...                         ...\n",
       "569545       mutants          8                       x-men\n",
       "569755  fronkonsteen          7          young frankenstein\n",
       "569861         mmmmm         10          young frankenstein\n",
       "569974      delbruck          6          young frankenstein\n",
       "570123          zulu          7                   zulu dawn\n",
       "\n",
       "[4959 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_dialogues_entire_dataset = df_word_dialogues_entire_dataset.drop_duplicates(subset=\"Word\")\n",
    "\n",
    "# Extract only the \"word\" and \"freq\" columns\n",
    "df_word_dialogues_entire_dataset = df_word_dialogues_entire_dataset[[\"Word\", \"Frequency\",\"Movie_Name\"]]\n",
    "\n",
    "df_word_dialogues_entire_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d8d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_dialogues_entire_dataset.to_json('wordfreq.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13ec7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = \"C:\\\\Users\\\\DELL\\\\Desktop\\\\VisProject\\\\wordfreq.csv\"\n",
    "\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_word_dialogues_entire_dataset.to_csv(wf, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13617d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'character'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m df_word_dialogues_entire_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_word_dialogues_entire_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Group by character and calculate average sentiment\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m character_sentiments \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharacter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the aggregated emotions for each character\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(character_sentiments)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7712\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7707\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m   7709\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[0;32m   7710\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[0;32m   7711\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[1;32m-> 7712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7715\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7718\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7720\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   7721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:882\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    880\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'character'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4058754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad570e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
